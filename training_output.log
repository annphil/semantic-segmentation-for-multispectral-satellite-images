2025-11-10 14:24:10.023723: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-11-10 14:24:10.081110: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 AVX512_FP16 AVX_VNNI AMX_TILE AMX_INT8 AMX_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2025-11-10 14:24:11.007076: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
I0000 00:00:1762784676.074626  105899 gpu_device.cc:2020] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 46669 MB memory:  -> device: 0, name: NVIDIA RTX 6000 Ada Generation, pci bus id: 0000:17:00.0, compute capability: 8.9
I0000 00:00:1762784676.075438  105899 gpu_device.cc:2020] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 46672 MB memory:  -> device: 1, name: NVIDIA RTX 6000 Ada Generation, pci bus id: 0000:63:00.0, compute capability: 8.9
I0000 00:00:1762784676.076104  105899 gpu_device.cc:2020] Created device /job:localhost/replica:0/task:0/device:GPU:2 with 46672 MB memory:  -> device: 2, name: NVIDIA RTX 6000 Ada Generation, pci bus id: 0000:e1:00.0, compute capability: 8.9
Num GPUs Available: 3
TensorFlow version: 2.20.0
Built with CUDA: True
Reading images
Loaded X_train shape: (1713, 16, 256, 256)
Loaded Y_train shape: (1713, 256, 256)
Loaded 1284 training images
Loaded 429 validation images
Done reading images
Training images: 1284
Validation images: 429
Sample image shape: (256, 256, 16)
Sample mask shape: (256, 256, 1)
Started training
Training data shape: (1284, 256, 256, 16), (1284, 256, 256, 1)
Validation data shape: (429, 256, 256, 16), (429, 256, 256, 1)
Model: "functional"
┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓
┃ Layer (type)        ┃ Output Shape      ┃    Param # ┃ Connected to      ┃
┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩
│ input_layer         │ (None, 256, 256,  │          0 │ -                 │
│ (InputLayer)        │ 16)               │            │                   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ conv2d (Conv2D)     │ (None, 256, 256,  │      4,640 │ input_layer[0][0] │
│                     │ 32)               │            │                   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ conv2d_1 (Conv2D)   │ (None, 256, 256,  │      9,248 │ conv2d[0][0]      │
│                     │ 32)               │            │                   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ max_pooling2d       │ (None, 128, 128,  │          0 │ conv2d_1[0][0]    │
│ (MaxPooling2D)      │ 32)               │            │                   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ batch_normalization │ (None, 128, 128,  │        128 │ max_pooling2d[0]… │
│ (BatchNormalizatio… │ 32)               │            │                   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ conv2d_2 (Conv2D)   │ (None, 128, 128,  │     18,496 │ batch_normalizat… │
│                     │ 64)               │            │                   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ conv2d_3 (Conv2D)   │ (None, 128, 128,  │     36,928 │ conv2d_2[0][0]    │
│                     │ 64)               │            │                   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ max_pooling2d_1     │ (None, 64, 64,    │          0 │ conv2d_3[0][0]    │
│ (MaxPooling2D)      │ 64)               │            │                   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ dropout (Dropout)   │ (None, 64, 64,    │          0 │ max_pooling2d_1[… │
│                     │ 64)               │            │                   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ batch_normalizatio… │ (None, 64, 64,    │        256 │ dropout[0][0]     │
│ (BatchNormalizatio… │ 64)               │            │                   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ conv2d_4 (Conv2D)   │ (None, 64, 64,    │     73,856 │ batch_normalizat… │
│                     │ 128)              │            │                   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ conv2d_5 (Conv2D)   │ (None, 64, 64,    │    147,584 │ conv2d_4[0][0]    │
│                     │ 128)              │            │                   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ max_pooling2d_2     │ (None, 32, 32,    │          0 │ conv2d_5[0][0]    │
│ (MaxPooling2D)      │ 128)              │            │                   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ dropout_1 (Dropout) │ (None, 32, 32,    │          0 │ max_pooling2d_2[… │
│                     │ 128)              │            │                   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ batch_normalizatio… │ (None, 32, 32,    │        512 │ dropout_1[0][0]   │
│ (BatchNormalizatio… │ 128)              │            │                   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ conv2d_6 (Conv2D)   │ (None, 32, 32,    │    295,168 │ batch_normalizat… │
│                     │ 256)              │            │                   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ conv2d_7 (Conv2D)   │ (None, 32, 32,    │    590,080 │ conv2d_6[0][0]    │
│                     │ 256)              │            │                   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ max_pooling2d_3     │ (None, 16, 16,    │          0 │ conv2d_7[0][0]    │
│ (MaxPooling2D)      │ 256)              │            │                   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ dropout_2 (Dropout) │ (None, 16, 16,    │          0 │ max_pooling2d_3[… │
│                     │ 256)              │            │                   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ batch_normalizatio… │ (None, 16, 16,    │      1,024 │ dropout_2[0][0]   │
│ (BatchNormalizatio… │ 256)              │            │                   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ conv2d_8 (Conv2D)   │ (None, 16, 16,    │  1,180,160 │ batch_normalizat… │
│                     │ 512)              │            │                   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ conv2d_9 (Conv2D)   │ (None, 16, 16,    │  2,359,808 │ conv2d_8[0][0]    │
│                     │ 512)              │            │                   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ max_pooling2d_4     │ (None, 8, 8, 512) │          0 │ conv2d_9[0][0]    │
│ (MaxPooling2D)      │                   │            │                   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ dropout_3 (Dropout) │ (None, 8, 8, 512) │          0 │ max_pooling2d_4[… │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ conv2d_10 (Conv2D)  │ (None, 8, 8,      │  4,719,616 │ dropout_3[0][0]   │
│                     │ 1024)             │            │                   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ conv2d_11 (Conv2D)  │ (None, 8, 8,      │  9,438,208 │ conv2d_10[0][0]   │
│                     │ 1024)             │            │                   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ conv2d_transpose    │ (None, 16, 16,    │  2,097,664 │ conv2d_11[0][0]   │
│ (Conv2DTranspose)   │ 512)              │            │                   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ concatenate         │ (None, 16, 16,    │          0 │ conv2d_transpose… │
│ (Concatenate)       │ 1024)             │            │ conv2d_9[0][0]    │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ batch_normalizatio… │ (None, 16, 16,    │      4,096 │ concatenate[0][0] │
│ (BatchNormalizatio… │ 1024)             │            │                   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ conv2d_12 (Conv2D)  │ (None, 16, 16,    │  4,719,104 │ batch_normalizat… │
│                     │ 512)              │            │                   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ conv2d_13 (Conv2D)  │ (None, 16, 16,    │  2,359,808 │ conv2d_12[0][0]   │
│                     │ 512)              │            │                   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ dropout_4 (Dropout) │ (None, 16, 16,    │          0 │ conv2d_13[0][0]   │
│                     │ 512)              │            │                   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ conv2d_transpose_1  │ (None, 32, 32,    │    524,544 │ dropout_4[0][0]   │
│ (Conv2DTranspose)   │ 256)              │            │                   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ concatenate_1       │ (None, 32, 32,    │          0 │ conv2d_transpose… │
│ (Concatenate)       │ 512)              │            │ conv2d_7[0][0]    │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ batch_normalizatio… │ (None, 32, 32,    │      2,048 │ concatenate_1[0]… │
│ (BatchNormalizatio… │ 512)              │            │                   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ conv2d_14 (Conv2D)  │ (None, 32, 32,    │  1,179,904 │ batch_normalizat… │
│                     │ 256)              │            │                   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ conv2d_15 (Conv2D)  │ (None, 32, 32,    │    590,080 │ conv2d_14[0][0]   │
│                     │ 256)              │            │                   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ dropout_5 (Dropout) │ (None, 32, 32,    │          0 │ conv2d_15[0][0]   │
│                     │ 256)              │            │                   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ conv2d_transpose_2  │ (None, 64, 64,    │    131,200 │ dropout_5[0][0]   │
│ (Conv2DTranspose)   │ 128)              │            │                   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ concatenate_2       │ (None, 64, 64,    │          0 │ conv2d_transpose… │
│ (Concatenate)       │ 256)              │            │ conv2d_5[0][0]    │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ batch_normalizatio… │ (None, 64, 64,    │      1,024 │ concatenate_2[0]… │
│ (BatchNormalizatio… │ 256)              │            │                   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ conv2d_16 (Conv2D)  │ (None, 64, 64,    │    295,040 │ batch_normalizat… │
│                     │ 128)              │            │                   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ conv2d_17 (Conv2D)  │ (None, 64, 64,    │    147,584 │ conv2d_16[0][0]   │
│                     │ 128)              │            │                   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ dropout_6 (Dropout) │ (None, 64, 64,    │          0 │ conv2d_17[0][0]   │
│                     │ 128)              │            │                   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ conv2d_transpose_3  │ (None, 128, 128,  │     32,832 │ dropout_6[0][0]   │
│ (Conv2DTranspose)   │ 64)               │            │                   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ concatenate_3       │ (None, 128, 128,  │          0 │ conv2d_transpose… │
│ (Concatenate)       │ 128)              │            │ conv2d_3[0][0]    │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ batch_normalizatio… │ (None, 128, 128,  │        512 │ concatenate_3[0]… │
│ (BatchNormalizatio… │ 128)              │            │                   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ conv2d_18 (Conv2D)  │ (None, 128, 128,  │     73,792 │ batch_normalizat… │
│                     │ 64)               │            │                   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ conv2d_19 (Conv2D)  │ (None, 128, 128,  │     36,928 │ conv2d_18[0][0]   │
│                     │ 64)               │            │                   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ dropout_7 (Dropout) │ (None, 128, 128,  │          0 │ conv2d_19[0][0]   │
│                     │ 64)               │            │                   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ conv2d_transpose_4  │ (None, 256, 256,  │      8,224 │ dropout_7[0][0]   │
│ (Conv2DTranspose)   │ 32)               │            │                   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ concatenate_4       │ (None, 256, 256,  │          0 │ conv2d_transpose… │
│ (Concatenate)       │ 64)               │            │ conv2d_1[0][0]    │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ conv2d_20 (Conv2D)  │ (None, 256, 256,  │     18,464 │ concatenate_4[0]… │
│                     │ 32)               │            │                   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ conv2d_21 (Conv2D)  │ (None, 256, 256,  │      9,248 │ conv2d_20[0][0]   │
│                     │ 32)               │            │                   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ conv2d_22 (Conv2D)  │ (None, 256, 256,  │         33 │ conv2d_21[0][0]   │
│                     │ 1)                │            │                   │
└─────────────────────┴───────────────────┴────────────┴───────────────────┘
 Total params: 31,107,841 (118.67 MB)
 Trainable params: 31,103,041 (118.65 MB)
 Non-trainable params: 4,800 (18.75 KB)
None
Epoch 1/50
2025-11-10 14:24:48.836473: I external/local_xla/xla/service/service.cc:163] XLA service 0x7fc9c0002ee0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2025-11-10 14:24:48.836492: I external/local_xla/xla/service/service.cc:171]   StreamExecutor device (0): NVIDIA RTX 6000 Ada Generation, Compute Capability 8.9
2025-11-10 14:24:48.836494: I external/local_xla/xla/service/service.cc:171]   StreamExecutor device (1): NVIDIA RTX 6000 Ada Generation, Compute Capability 8.9
2025-11-10 14:24:48.836495: I external/local_xla/xla/service/service.cc:171]   StreamExecutor device (2): NVIDIA RTX 6000 Ada Generation, Compute Capability 8.9
2025-11-10 14:24:48.999725: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2025-11-10 14:24:50.441554: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:473] Loaded cuDNN version 91500
I0000 00:00:1762784721.377562  106422 device_compiler.h:196] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2025-11-10 14:25:42.591893: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'input_reduce_fusion_13', 8 bytes spill stores, 8 bytes spill loads
ptxas warning : Registers are spilled to local memory in function 'input_reduce_fusion_14', 16 bytes spill stores, 16 bytes spill loads
ptxas warning : Registers are spilled to local memory in function 'input_reduce_fusion_15', 12 bytes spill stores, 12 bytes spill loads

21/21 - 73s - 3s/step - dice_coefficient: 0.3894 - loss: 0.6192 - val_dice_coefficient: 0.4043 - val_loss: 0.5960
Epoch 2/50
21/21 - 8s - 363ms/step - dice_coefficient: 0.4372 - loss: 0.5576 - val_dice_coefficient: 0.4179 - val_loss: 0.5818
Epoch 3/50
21/21 - 7s - 341ms/step - dice_coefficient: 0.4585 - loss: 0.5350 - val_dice_coefficient: 0.4635 - val_loss: 0.5370
Epoch 4/50
21/21 - 6s - 285ms/step - dice_coefficient: 0.4763 - loss: 0.5287 - val_dice_coefficient: 0.4419 - val_loss: 0.5578
Epoch 5/50
21/21 - 6s - 280ms/step - dice_coefficient: 0.4611 - loss: 0.5339 - val_dice_coefficient: 0.4489 - val_loss: 0.5512
Epoch 6/50
21/21 - 6s - 287ms/step - dice_coefficient: 0.4632 - loss: 0.5310 - val_dice_coefficient: 0.4514 - val_loss: 0.5500
Epoch 7/50
21/21 - 6s - 281ms/step - dice_coefficient: 0.4670 - loss: 0.5404 - val_dice_coefficient: 0.4454 - val_loss: 0.5546
Epoch 8/50
21/21 - 7s - 355ms/step - dice_coefficient: 0.4748 - loss: 0.5312 - val_dice_coefficient: 0.4661 - val_loss: 0.5347
Epoch 9/50
21/21 - 8s - 370ms/step - dice_coefficient: 0.4686 - loss: 0.5225 - val_dice_coefficient: 0.4689 - val_loss: 0.5320
Epoch 10/50
21/21 - 8s - 366ms/step - dice_coefficient: 0.4654 - loss: 0.5173 - val_dice_coefficient: 0.4846 - val_loss: 0.5164
Epoch 11/50
21/21 - 6s - 274ms/step - dice_coefficient: 0.5068 - loss: 0.5052 - val_dice_coefficient: 0.4694 - val_loss: 0.5315
Epoch 12/50
21/21 - 6s - 274ms/step - dice_coefficient: 0.4759 - loss: 0.5101 - val_dice_coefficient: 0.4817 - val_loss: 0.5203
Epoch 13/50
21/21 - 6s - 278ms/step - dice_coefficient: 0.5012 - loss: 0.5032 - val_dice_coefficient: 0.4646 - val_loss: 0.5375
Epoch 14/50
21/21 - 6s - 278ms/step - dice_coefficient: 0.4629 - loss: 0.5205 - val_dice_coefficient: 0.3889 - val_loss: 0.6106
Epoch 15/50
21/21 - 8s - 368ms/step - dice_coefficient: 0.4931 - loss: 0.5108 - val_dice_coefficient: 0.4932 - val_loss: 0.5081
Epoch 16/50
21/21 - 6s - 275ms/step - dice_coefficient: 0.4743 - loss: 0.5233 - val_dice_coefficient: 0.4627 - val_loss: 0.5371
Epoch 17/50
21/21 - 6s - 280ms/step - dice_coefficient: 0.4701 - loss: 0.5171 - val_dice_coefficient: 0.4810 - val_loss: 0.5198
Epoch 18/50
21/21 - 6s - 278ms/step - dice_coefficient: 0.4978 - loss: 0.5011 - val_dice_coefficient: 0.4707 - val_loss: 0.5291
Epoch 19/50
21/21 - 7s - 356ms/step - dice_coefficient: 0.5056 - loss: 0.4992 - val_dice_coefficient: 0.4948 - val_loss: 0.5062
Epoch 20/50
21/21 - 6s - 283ms/step - dice_coefficient: 0.4899 - loss: 0.5020 - val_dice_coefficient: 0.4812 - val_loss: 0.5191
Epoch 21/50
21/21 - 6s - 283ms/step - dice_coefficient: 0.5146 - loss: 0.4967 - val_dice_coefficient: 0.4673 - val_loss: 0.5314
Epoch 22/50
21/21 - 6s - 276ms/step - dice_coefficient: 0.4972 - loss: 0.5009 - val_dice_coefficient: 0.4861 - val_loss: 0.5145
Epoch 23/50
21/21 - 6s - 276ms/step - dice_coefficient: 0.5058 - loss: 0.4933 - val_dice_coefficient: 0.4680 - val_loss: 0.5315
Epoch 24/50
21/21 - 6s - 278ms/step - dice_coefficient: 0.4734 - loss: 0.5197 - val_dice_coefficient: 0.4622 - val_loss: 0.5384
Epoch 25/50
21/21 - 6s - 276ms/step - dice_coefficient: 0.4943 - loss: 0.5120 - val_dice_coefficient: 0.4923 - val_loss: 0.5089
Epoch 26/50
21/21 - 6s - 277ms/step - dice_coefficient: 0.4895 - loss: 0.5002 - val_dice_coefficient: 0.4798 - val_loss: 0.5209
Epoch 27/50
21/21 - 6s - 278ms/step - dice_coefficient: 0.5081 - loss: 0.4993 - val_dice_coefficient: 0.4771 - val_loss: 0.5243
Epoch 28/50
21/21 - 8s - 368ms/step - dice_coefficient: 0.5064 - loss: 0.4984 - val_dice_coefficient: 0.4973 - val_loss: 0.5045
Epoch 29/50
21/21 - 6s - 283ms/step - dice_coefficient: 0.5141 - loss: 0.4946 - val_dice_coefficient: 0.4744 - val_loss: 0.5250
Epoch 30/50
21/21 - 8s - 379ms/step - dice_coefficient: 0.5130 - loss: 0.4933 - val_dice_coefficient: 0.5036 - val_loss: 0.4973
Epoch 31/50
21/21 - 6s - 275ms/step - dice_coefficient: 0.4987 - loss: 0.4921 - val_dice_coefficient: 0.5004 - val_loss: 0.5012
Epoch 32/50
21/21 - 8s - 368ms/step - dice_coefficient: 0.4954 - loss: 0.4917 - val_dice_coefficient: 0.5051 - val_loss: 0.4966
Epoch 33/50
21/21 - 6s - 274ms/step - dice_coefficient: 0.5083 - loss: 0.4921 - val_dice_coefficient: 0.4931 - val_loss: 0.5082
Epoch 34/50
21/21 - 6s - 272ms/step - dice_coefficient: 0.5017 - loss: 0.4879 - val_dice_coefficient: 0.4759 - val_loss: 0.5255
Epoch 35/50
21/21 - 6s - 277ms/step - dice_coefficient: 0.5142 - loss: 0.4852 - val_dice_coefficient: 0.4931 - val_loss: 0.5081
Epoch 36/50
21/21 - 6s - 276ms/step - dice_coefficient: 0.5110 - loss: 0.4842 - val_dice_coefficient: 0.4940 - val_loss: 0.5061
Epoch 37/50
21/21 - 6s - 274ms/step - dice_coefficient: 0.4909 - loss: 0.5052 - val_dice_coefficient: 0.3131 - val_loss: 0.6822
Epoch 38/50
21/21 - 6s - 274ms/step - dice_coefficient: 0.4820 - loss: 0.5186 - val_dice_coefficient: 0.3471 - val_loss: 0.6530
Epoch 39/50
21/21 - 6s - 275ms/step - dice_coefficient: 0.4799 - loss: 0.5171 - val_dice_coefficient: 0.4846 - val_loss: 0.5165
Epoch 40/50
21/21 - 6s - 275ms/step - dice_coefficient: 0.5077 - loss: 0.4941 - val_dice_coefficient: 0.4819 - val_loss: 0.5185
Epoch 41/50
21/21 - 6s - 275ms/step - dice_coefficient: 0.5149 - loss: 0.4953 - val_dice_coefficient: 0.4705 - val_loss: 0.5310
Epoch 42/50
21/21 - 6s - 272ms/step - dice_coefficient: 0.5225 - loss: 0.4898 - val_dice_coefficient: 0.4937 - val_loss: 0.5077
Epoch 43/50
21/21 - 6s - 275ms/step - dice_coefficient: 0.5078 - loss: 0.4832 - val_dice_coefficient: 0.5006 - val_loss: 0.5018
Epoch 44/50
21/21 - 8s - 369ms/step - dice_coefficient: 0.4987 - loss: 0.4876 - val_dice_coefficient: 0.5071 - val_loss: 0.4943
Epoch 45/50
21/21 - 6s - 273ms/step - dice_coefficient: 0.5126 - loss: 0.4836 - val_dice_coefficient: 0.4815 - val_loss: 0.5180
Epoch 46/50
21/21 - 6s - 275ms/step - dice_coefficient: 0.5131 - loss: 0.4859 - val_dice_coefficient: 0.4854 - val_loss: 0.5171
Epoch 47/50
21/21 - 6s - 275ms/step - dice_coefficient: 0.5022 - loss: 0.4951 - val_dice_coefficient: 0.4874 - val_loss: 0.5134
Epoch 48/50
21/21 - 6s - 273ms/step - dice_coefficient: 0.5139 - loss: 0.4895 - val_dice_coefficient: 0.4996 - val_loss: 0.5017
Epoch 49/50
21/21 - 6s - 273ms/step - dice_coefficient: 0.4916 - loss: 0.4930 - val_dice_coefficient: 0.4899 - val_loss: 0.5115
Epoch 50/50
21/21 - 6s - 275ms/step - dice_coefficient: 0.5143 - loss: 0.4855 - val_dice_coefficient: 0.4926 - val_loss: 0.5076
